# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
Main Training Script
Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ
"""

import os
import sys
import argparse
import torch

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ù…ÙƒØªØ¨Ø©
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from ai_core.config import ModelConfig
from ai_core.tokenizer import ArabicTokenizer
from ai_core.model import TransformerModel
from ai_core.trainer import Trainer, ConversationDataset, ChatBot


def train_tokenizer(data_dir: str, vocab_size: int = 10000, output_dir: str = "ai_data/vocab"):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    print("\n" + "="*50)
    print("ğŸ”¤ ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ")
    print("="*50)
    
    # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
    import json
    from pathlib import Path
    
    all_texts = []
    data_path = Path(data_dir)
    
    for json_file in data_path.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if "conversations" in data:
                for conv in data["conversations"]:
                    if "user" in conv:
                        all_texts.append(conv["user"])
                    if "assistant" in conv:
                        all_texts.append(conv["assistant"])
    
    print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØµÙˆØµ: {len(all_texts)}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    tokenizer = ArabicTokenizer(vocab_size=vocab_size)
    tokenizer.train(all_texts, verbose=True)
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    os.makedirs(output_dir, exist_ok=True)
    tokenizer.save(output_dir)
    
    return tokenizer


def train_model(
    tokenizer: ArabicTokenizer,
    data_dir: str,
    model_size: str = "small",
    epochs: int = 10,
    batch_size: int = 8,
    learning_rate: float = 1e-4,
    output_dir: str = "ai_data/models"
):
    """
    ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    """
    print("\n" + "="*50)
    print("ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    print("="*50)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    if model_size == "small":
        config = ModelConfig.small()
    elif model_size == "medium":
        config = ModelConfig.medium()
    elif model_size == "large":
        config = ModelConfig.large()
    else:
        config = ModelConfig.small()
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config.vocab_size = len(tokenizer)
    config.epochs = epochs
    config.batch_size = batch_size
    config.learning_rate = learning_rate
    config.model_dir = output_dir
    
    print(f"\nğŸ“Š Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
    print(f"   Ø§Ù„Ø­Ø¬Ù…: {model_size}")
    print(f"   Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {config.vocab_size}")
    print(f"   Ø§Ù„Ø·Ø¨Ù‚Ø§Øª: {config.n_layers}")
    print(f"   Attention Heads: {config.n_heads}")
    print(f"   d_model: {config.d_model}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = TransformerModel(config)
    print(f"\nğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {model.count_parameters():,}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    dataset = ConversationDataset(
        data_dir=data_dir,
        tokenizer=tokenizer,
        max_length=config.max_seq_length
    )
    
    if len(dataset) == 0:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨!")
        return None, None
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        config=config,
        output_dir=output_dir
    )
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    history = trainer.train(
        train_dataset=dataset,
        epochs=epochs,
        batch_size=batch_size,
        save_every=2
    )
    
    # Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config.save(os.path.join(output_dir, "config.json"))
    
    return model, trainer


def interactive_chat(model_path: str, tokenizer_path: str):
    """
    ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    """
    print("\n" + "="*50)
    print("ğŸ’¬ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©")
    print("="*50)
    print("Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ø£Ùˆ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬")
    print("Ø§ÙƒØªØ¨ 'Ù…Ø³Ø­' Ù„Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
    print("="*50 + "\n")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    tokenizer = ArabicTokenizer.load(tokenizer_path)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = TransformerModel.load(model_path)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    chatbot = ChatBot(model, tokenizer)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ Ø£Ù†Øª: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit', 'q']:
                print("\nğŸ‘‹ Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©!")
                break
            
            if user_input.lower() in ['Ù…Ø³Ø­', 'clear', 'reset']:
                chatbot.reset()
                print("ğŸ—‘ï¸ ØªÙ… Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
                continue
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯
            response = chatbot.chat(
                user_input,
                max_length=100,
                temperature=0.8
            )
            
            print(f"\nğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©!")
            break
        except Exception as e:
            print(f"\nâŒ Ø®Ø·Ø£: {e}")


def main():
    parser = argparse.ArgumentParser(description="Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ")
    
    parser.add_argument(
        "--mode",
        type=str,
        choices=["train", "chat", "full"],
        default="full",
        help="ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„: train (ØªØ¯Ø±ÙŠØ¨), chat (Ù…Ø­Ø§Ø¯Ø«Ø©), full (ÙƒØ§Ù…Ù„)"
    )
    
    parser.add_argument(
        "--data-dir",
        type=str,
        default="ai_data/conversations",
        help="Ù…Ø³Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"
    )
    
    parser.add_argument(
        "--model-dir",
        type=str,
        default="ai_data/models",
        help="Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
    )
    
    parser.add_argument(
        "--vocab-dir",
        type=str,
        default="ai_data/vocab",
        help="Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"
    )
    
    parser.add_argument(
        "--model-size",
        type=str,
        choices=["small", "medium", "large"],
        default="small",
        help="Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=10,
        help="Ø¹Ø¯Ø¯ Ø­Ù‚Ø¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©"
    )
    
    parser.add_argument(
        "--vocab-size",
        type=int,
        default=10000,
        help="Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"
    )
    
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-4,
        help="Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…"
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸ¤– Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ - BI Distor")
    print("="*60)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† CUDA
    if torch.cuda.is_available():
        print(f"âœ… GPU Ù…ØªØ§Ø­: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ GPU ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU")
    
    if args.mode in ["train", "full"]:
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        tokenizer = train_tokenizer(
            data_dir=args.data_dir,
            vocab_size=args.vocab_size,
            output_dir=args.vocab_dir
        )
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model, trainer = train_model(
            tokenizer=tokenizer,
            data_dir=args.data_dir,
            model_size=args.model_size,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            output_dir=args.model_dir
        )
        
        if model is None:
            print("âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
            return
    
    if args.mode in ["chat", "full"]:
        model_path = os.path.join(args.model_dir, "best_model.pt")
        
        if not os.path.exists(model_path):
            model_path = os.path.join(args.model_dir, "final_model.pt")
        
        if os.path.exists(model_path):
            interactive_chat(model_path, args.vocab_dir)
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨!")
            print(f"   Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {model_path}")


if __name__ == "__main__":
    main()
